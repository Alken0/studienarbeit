%!TEX root = ./main.tex

@article{example,
	author = {{Author Name1, Author Name 2}},
	title = {Example title},
	institution = {my institution},
	date = {2021-12-30},
}

@reference{dataset:four-shapes,
	editor = {Smeschke},
	title = {Four Shapes (16.000 images of four basic shapes (star, circle, square, triangle)},
	date = {2017-11-21},
	OPTurl = {https://www.kaggle.com/smeschke/four-shapes},
	OPTurldate = {2021-10-31},
}


@thesis{dataset:2d-geometric-shapes-dataset,
	author = {Anas El Korchi, Youssef Ghanou},
	title = {2D geometric shapes dataset for machine learning and pattern recognition},
	type = {Data Article},
	institution = {University Moulay Ismail of Meknes, Marocco},
	date = {2020-07-05},
	OPTurl = {https://www.sciencedirect.com/science/article/pii/S2352340920309847},
	OPTurldate = {2021-10-31},
}

@thesis{are-gans-created-equally,
	author = {{Mario Lucic, Karol Kurach, Marcin Michalski, Olivier Bousquet, Sylvain Gelly}},
	title = {Are GANs Created Equal? A Large-Scale Study},
	institution = {Google Brains},
	date = {2018-10-29},
	OPTurl = {https://arxiv.org/pdf/1711.10337.pdf},
	OPTurldate = {2021-10-31},
}

@thesis{structural-similarity-index,
	author = {{Zhou Wnag, Alan Conrad Bovik, Hamid Rahim Sheikh, Eero P. Simoncelli}},
	title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
	institution = {IEEE},
	date = {2004-04-04},
	OPTurl = {https://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf},
	OPTurldate = {2021-11-04},
}

@online{tensorflow,
	author = {Google},
	title = {Tensorflow},
	year = 2021,
	url = {https://www.tensorflow.org},
	urldate = {2021-11-04},
}

@online{common-loss,
	author = {Vitaly Bushaev},
	title = {Common Loss functions in machine learning},
	year = 2018,
	url = {https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23},
	urldate = {2022-01-29}
}

@book{russel-norvig,
	author = {{Stuart J. Russel, Peter Norvig}},
	title = {Artificial Intelligence - A Modern Approach (Third Edition)},
	date = {2009},
	isbn = {978-0-13-604259-4},
	OPTchapter = {chapter},
}

% VON HIER AN NOCHMAL ÜBERARBEITEN
@online{hyperparameters-gan-using-genetic-algorithm,
	author = {{Fajr Ibrahem Alarsan, Mamoon Younes}},
	title = {Best Selection of Generative Adversarial Networks
	Hyper-Parameters using Genetic Algorithm},
	year = 2021,
	url = {https://link.springer.com/article/10.1007/s42979-021-00689-3},
	urldate = {2022-01-29}
}

@online{hyperparameters-search,
	author = {{Towards Data Science}},
	title = {Understanding Hyperparameters and its Optimisation techniques},
	year = 2018,
	url = {https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568},
	urldate = {2022-01-29}
}



@online{hyperparameters-what-how,
	author = {{Pranoy Radhakrishnan}},
	title = {What are Hyperparameters ? and How to tune the Hyperparameters in a Deep Neural Network?},
	year = 2017,
	url = {https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a},
	urldate = {2022-01-29}
}

@online{hyperparameters-search-in-machine-learning,
	author = {{Marc Claesen, Bart De Moor}},
	title = {Hyperparameter Search in Machine Learning},
	year = 2015,
	url = {https://arxiv.org/pdf/1502.02127.pdf},
	urldate = {2022-01-29}
}

@online{hyperparameters-grid-search-tutorial,
	author = {{Marc Claesen, Bart De Moor}},
	title = {How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras},
	year = 2016,
	url = {https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/},
	urldate = {2022-01-29}
}

@ARTICLE{lossfunction-opportunities-and-challenges,
	author={Pan, Zhaoqing and Yu, Weijie and Wang, Bosi and Xie, Haoran and Sheng, Victor S. and Lei, Jianjun and Kwong, Sam},
	journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
	title={Loss Functions of Generative Adversarial Networks (GANs): Opportunities and Challenges}, 
	year={2020},
	volume={4},
	number={4},
	pages={500-522},
	doi={10.1109/TETCI.2020.2991774}
}

@online{learningrate-how-to-configure,
	author = {{Jason Brownlee}},
	title = {How to Configure the Learning Rate When Training Deep Learning Neural Networks},
	year = 2019,
	url = {https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/},
	urldate = {2022-01-29}
}

@online{adam,
	author = {{Jason Brownlee}},
	title = {Gentle Introduction to the Adam Optimization Algorithm for Deep Learning},
	year = 2017,
	url = {https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/},
	urldate = {2022-01-29}
}

@online{nodes-how-to-configure,
	author = {{Jason Brownlee}},
	title = {How to Configure the Number of Layers and Nodes in a Neural Network},
	year = 2018,
	url = {https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/},
	urldate = {2022-01-29}
}

@article{nodes-hiddenlayers-how-many,
	author = { D.   Stathakis },
	title = {How many hidden layers and nodes?},
	journal = {International Journal of Remote Sensing},
	volume = {30},
	number = {8},
	pages = {2133-2147},
	year  = {2009},
	publisher = {Taylor & Francis},
	doi = {10.1080/01431160802549278},
	URL = {https://doi.org/10.1080/01431160802549278},
	eprint = {https://doi.org/10.1080/01431160802549278}
}

@misc{gan-original-paper,
	doi = {10.48550/ARXIV.1406.2661},
	url = {https://arxiv.org/abs/1406.2661},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Generative Adversarial Networks},
	publisher = {arXiv},
	year = {2014},
	copyright = {arXiv.org perpetual, non-exclusive license}
}




@misc{mirza2014conditional,
	author = {Mirza, Mehdi and Osindero, Simon},
	biburl = {https://www.bibsonomy.org/bibtex/2a4426d639ebb30270839ad347bcfb999/achakraborty},
	description = {Conditional Generative Adversarial Nets},
	interhash = {efbbaeaebb1ea8d88264d258624d364c},
	intrahash = {a4426d639ebb30270839ad347bcfb999},
	keywords = {2014 GAN deep-learning machine-learning neural-networks},
	timestamp = {2017-10-04T17:14:40.000+0200},
	title = {Conditional Generative Adversarial Nets},
	url = {http://arxiv.org/abs/1411.1784},
	year = 2014
}

@online{inspiration-dense-and-dc-gan,
	author = {{mafda}},
	title = {A (conditional) Generative Adversarial Network},
	year = 2019,
	url = {https://github.com/mafda/generative_adversarial_networks_101/blob/master/src/mnist/03_CGAN_MNIST.ipynb},
	urldate = {2022-01-29}
}


@misc{gan-conditional,
	doi = {10.48550/ARXIV.1411.1784},
	url = {https://arxiv.org/abs/1411.1784},
	author = {Mirza, Mehdi and Osindero, Simon},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Conditional Generative Adversarial Nets},
	publisher = {arXiv},
	year = {2014},
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@online{gan-minimax,
	author = {{Steven Van Vaerenbergh, Ian Goodfellow}},
	title = {Ian Goodfellow: Generative Adversarial Networks (NIPS 2016 tutorial)},
	year = 2018,
	url = {https://youtu.be/HGYYEUSm-0Q?t=2004},
	urldate = {2022-01-29}
}

@Article{gan-application-upscaling,
	author={Sharma, Prasen Kumar
	and Basavaraju, Sathisha
	and Sur, Arijit},
	title={High-resolution image de-raining using conditional GAN with sub-pixel upscaling},
	journal={Multimedia Tools and Applications},
	year={2021},
	month={Jan},
	day={01},
	abstract={High-quality image de-raining is a challenging task that has been given considerable importance in recent times. To begin with, this problem is modeled as an image decomposition task where a rainy image is decomposed into the rain-free background and the associated rain streak map. Most of the existing methods have been successful in removing the rain-streaks but fails to restore the image quality, which is degraded due to noise removal. This paper proposes a novel architecture called High-Resolution Image De-Raining using Conditional Generative Adversarial Networks (HRID-GAN) to generate a de-rained image with minimal artifacts and better visual quality. Extensive experiments on publicly available synthetic as well as real-world datasets show a substantial improvement over the state-of-the-art methods SPANet (Wang et al. 2019) by  ∼ 2.43{\%} in PSNR and, DID-MDN (Zhang and Patel 2018) by  ∼ 2.43{\%},  ∼ 10.12{\%} and ID-CGAN (Zhang et al. 2017) by  ∼ 11.80{\%},  ∼ 34.70{\%} in SSIM and PSNR respectively.},
	issn={1573-7721},
	doi={10.1007/s11042-020-09642-7},
	url={https://doi.org/10.1007/s11042-020-09642-7},
	urldate = {2022-01-29}
}

@misc{gan-application-blending,
	doi = {10.48550/ARXIV.2110.11728},
	url = {https://arxiv.org/abs/2110.11728},
	author = {Liu, Mingcong and Li, Qiang and Qin, Zekui and Zhang, Guoxin and Wan, Pengfei and Zheng, Wen},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation},
	publisher = {arXiv},
	year = {2021},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{gan-application-augmenting-training-data,
	doi = {10.48550/ARXIV.1810.10863},
	url = {https://arxiv.org/abs/1810.10863},
	author = {Bowles, Christopher and Chen, Liang and Guerrero, Ricardo and Bentley, Paul and Gunn, Roger and Hammers, Alexander and Dickie, David Alexander and Hernández, Maria Valdés and Wardlaw, Joanna and Rueckert, Daniel},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks},
	publisher = {arXiv},
	year = {2018},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{gan-application-dna-optimizes-protein-functions,
	author={Gupta, Anvita
	and Zou, James},
	title={Feedback GAN for DNA optimizes protein functions},
	journal={Nature Machine Intelligence},
	year={2019},
	month={Feb},
	day={01},
	abstract={Generative adversarial networks (GANs) represent an attractive and novel approach to generate realistic data, such as genes, proteins or drugs, in synthetic biology. Here, we apply GANs to generate synthetic DNA sequences encoding for proteins of variable length. We propose a novel feedback-loop architecture, feedback GAN (FBGAN), to optimize the synthetic gene sequences for desired properties using an external function analyser. The proposed architecture also has the advantage that the analyser does not need to be differentiable. We apply the feedback-loop mechanism to two examples: generating synthetic genes coding for antimicrobial peptides, and optimizing synthetic genes for the secondary structure of their resulting peptides. A suite of metrics, calculated in silico, demonstrates that the GAN-generated proteins have desirable biophysical properties. The FBGAN architecture can also be used to optimize GAN-generated data points for useful properties in domains beyond genomics.},
	issn={2522-5839},
	doi={10.1038/s42256-019-0017-4},
	url={https://doi.org/10.1038/s42256-019-0017-4}
}

@misc{gan-application-audio-synthesis,
	doi = {10.48550/ARXIV.2106.02297},
	url = {https://arxiv.org/abs/2106.02297},
	author = {Kim, Ji-Hoon and Lee, Sang-Hoon and Lee, Ji-Hyun and Lee, Seong-Whan},
	keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Fre-GAN: Adversarial Frequency-consistent Audio Synthesis},
	publisher = {arXiv},
	year = {2021},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{hyperparameters-genetic-algorithm,
	author={Alarsan, Fajr Ibrahem
	and Younes, Mamoon},
	title={Best Selection of Generative Adversarial Networks Hyper-Parameters Using Genetic Algorithm},
	journal={SN Computer Science},
	year={2021},
	month={May},
	day={20},
	volume={2},
	number={4},
	pages={283},
	abstract={Generative adversarial networks (GANs) are most popular generative frameworks that have achieved compelling performance. They follow an adversarial approach where two deep models generator and discriminator compete with each other. In this paper, we propose a Generative Adversarial Network with best hyper-parameters selection to generate fake images for digit numbers 1--9 with generator and train discriminator to decide whereas the generated images are fake or true. Genetic algorithm (GA) technique was used to adapt GAN hyper-parameters, the resulted algorithm is named GANGA: generative adversarial network with genetic algorithm. The resulted algorithm has achieved high performance; it was able to get zero value of loss function for the generator and discriminator separately. Anaconda environment with tensorflow library facilitates was used; python as programming language was adapted with needed libraries. The implementation was done using MNIST dataset to validate the work. The proposed method is to let genetic algorithm choose best values of hyper-parameters depending on minimizing a cost function such as a loss function or maximizing accuracy function used to find best values of learning rate, batch normalization, number of neurons and a parameter of dropout layer.},
	issn={2661-8907},
	doi={10.1007/s42979-021-00689-3},
	url={https://doi.org/10.1007/s42979-021-00689-3}
}

@thesis{genetic-algorithms,
	author = {{L.B. Booker, D.E. Goldberg and J.H. Holland }},
	title = {Classifier Systems and Genetic Algorithms},
	year = 1989,
	institution = {The University of Michigan},
	date = {2004-04-04},
	url = {https://www.cs.us.es/cursos/ia2-2012/trabajos/BucketBrigade.pdf},
	urldate = {2021-11-04},
}


@book{hyperparameters-grid-search,
	author = {{Frank Hutter, Lars Kotthoff, Joaquin Vanschoren}},
	title = {Automated Machine Learning - Methods, Systems, Challenges},
	year = 2019,
	url = {https://link.springer.com/content/pdf/10.1007%2F978-3-030-05318-5.pdf},
	urldate = {2022-01-29}
}

@article{dataset:mnist,
	added-at = {2010-06-28T21:16:30.000+0200},
	author = {LeCun, Yann and Cortes, Corinna},
	biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
	groups = {public},
	howpublished = {http://yann.lecun.com/exdb/mnist/},
	interhash = {21b9d0558bd66279df9452562df6e6f3},
	intrahash = {935bad99fa1f65e03c25b315aa3c1032},
	keywords = {MSc _checked character_recognition mnist network neural},
	lastchecked = {2016-01-14 14:24:11},
	timestamp = {2016-07-12T19:25:30.000+0200},
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2022-01-29},
	year = 2010
}

@INPROCEEDINGS{dataset:image-net,
	author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
	booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
	title={ImageNet: A large-scale hierarchical image database}, 
	year={2009},
	volume={},
	number={},
	pages={248-255},
	doi={10.1109/CVPR.2009.5206848}
}


@article{dataset:lsun,
	Author = {Yu, Fisher and Zhang, Yinda and Song, Shuran and Seff, Ari and Xiao, Jianxiong},
	Title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},
	Journal = {arXiv preprint arXiv:1506.03365},
	Year = {2015}
}

@inproceedings{automatic-relevance-determination,
	author = {Wipf, David and Nagarajan, Srikantan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {A New View of Automatic Relevance Determination},
	url = {https://proceedings.neurips.cc/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf},
	volume = {20},
	year = {2008}
}

@misc{hyperparameters-search-comparison-focus-genetic,
	doi = {10.48550/ARXIV.1912.06059},
	url = {https://arxiv.org/abs/1912.06059},
	urldate = {2022-01-29},
	author = {Liashchynskyi, Petro and Liashchynskyi, Pavlo},
	keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS},
	publisher = {arXiv},
	year = {2019},
	copyright = {arXiv.org perpetual, non-exclusive license}
}



@online{hyperparameters-random-search,
	author = {{James Bergstra, Yoshua Bengio, Leon Bottou}},
	title = {Random Search for Hyper-Parameter Optimization},
	year = 2012,
	url = {https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
	urldate = {2022-01-29}
}

@misc{
	gan-landscape-losses-architectures-regularization-normalization,
	title={The {GAN} Landscape: Losses, Architectures, Regularization, and Normalization},
	author={Anonymous authors},
	year={2019},
	url={https://openreview.net/pdf?id=rkGG6s0qKQ},
}

@misc{hyperparameters-convolutional-gan,
	doi = {10.48550/ARXIV.1511.06434},
	url = {https://arxiv.org/abs/1511.06434},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	publisher = {arXiv},
	year = {2015},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{hyperparameters-using-genetic-algorithm-2,
	author={Nikbakht, Saeid
	and Anitescu, Cosmin
	and Rabczuk, Timon},
	title={Optimizing the neural network hyperparameters utilizing genetic algorithm},
	journal={Journal of Zhejiang University-SCIENCE A},
	year={2021},
	month={Jun},
	day={01},
	volume={22},
	number={6},
	pages={407-426},
	abstract={Neural networks (NNs), as one of the most robust and efficient machine learning methods, have been commonly used in solving several problems. However, choosing proper hyperparameters (e.g. the numbers of layers and neurons in each layer) has a significant influence on the accuracy of these methods. Therefore, a considerable number of studies have been carried out to optimize the NN hyperparameters. In this study, the genetic algorithm is applied to NN to find the optimal hyperparameters. Thus, the deep energy method, which contains a deep neural network, is applied first on a Timoshenko beam and a plate with a hole. Subsequently, the numbers of hidden layers, integration points, and neurons in each layer are optimized to reach the highest accuracy to predict the stress distribution through these structures. Thus, applying the proper optimization method on NN leads to significant increase in the NN prediction accuracy after conducting the optimization in various examples.},
	issn={1862-1775},
	doi={10.1631/jzus.A2000384},
	url={https://doi.org/10.1631/jzus.A2000384}
}



@online{img-gan,
	author = {{Thalles Silva}},
	title = {An intuitive introduction to Generative Adversarial Networks (GANs)},
	year = 2018,
	url = {https://www.freecodecamp.org/news/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394/},
	urldate = {2022-01-29}
}

@online{tutorial:tune-batch-size-analyticsvidhya,
	author = {{rendyk}},
	title = {Tuning the Hyperparameters and Layers of Neural Network Deep Learning},
	year = 2021,
	url = {https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning},
	urldate = {2022-01-29}
}

@online{tutorial:tune-batch-size-machinelearningmastery,
	author = {{Jason Brownlee}},
	title = {How to Control the Stability of Training Neural Networks With the Batch Size},
	year = 2019,
	url = {https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/},
	urldate = {2022-01-29}
}

@article{frechet-inception-distance,
	doi = {10.48550/ARXIV.1706.08500},
	url = {https://arxiv.org/abs/1706.08500},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	publisher = {arXiv},
	year = {2017},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ian-goodfellow-onesided-label-smoothing,
	doi = {10.48550/ARXIV.1606.03498},
	url = {https://arxiv.org/abs/1606.03498},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Improved Techniques for Training GANs},
	publisher = {arXiv},
	year = {2016},
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@online{tensorflow-gan-learn-step,
	author = {{Google}},
	title = {Deep Convolutional Generative Adversarial Network},
	year = 2022,
	url = {https://www.tensorflow.org/tutorials/generative/dcgan},
	urldate = {2022-01-29}
}

@online{hyperparams-search:framework-random-search,
	author = {{Tom O’Malley}},
	title = {Hyperparameter tuning with Keras Tuner},
	year = 2020,
	url = {https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html},
	urldate = {2022-01-29}
}

@online{hyperparams-search:framework-evolutionary-search,
	author = {{Ahmed Fawzy Gad}},
	title = {PyGAD - Python Genetic Algorithm!},
	year = 2020,
	url = {https://pygad.readthedocs.io/en/latest/},
	urldate = {2022-01-29}
}

@article{activation-function,
	author       = {Himanshu S},
	title        = { Activation  Functions : Sigmoid, tanh, ReLU, Leaky ReLU, PReLU, ELU, Threshold ReLU  and Softmax basics for Neural Networks and Deep Learning },
	howpublished = {\url{https://himanshuxd.medium.com/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e}},
	year         = {2019}
}

@inbook{learning-rate-most-important,
	author = {Y. Bengio},
	title = {Practical recommendations for gradient-based training of
	deep architectures},
	date = {2012},
	volume = {7700},
	pages = {437–478}
}

@online{tensorboard,
	author = {Google},
	title = {TensorBoard: TensorFlow's visualization toolkit},
	url = {https://www.tensorflow.org/tensorboard},
	urldate = {2022-04-19}
}

@online{pytorch-vs-tensorflow,
	author = {Vihar Kurama},
	title = {PyTorch vs. TensorFlow: Which Framework Is Best for Your Deep Learning Project?},
	date = {2021-12-01},
	url = {https://builtin.com/data-science/pytorch-vs-tensorflow},
	urldate = {2022-04-19}
}

@inproceedings{dl-framework-evaluation,
	author = {{Elshawi, Radwa} {Wahab, Abdul.} {Barnawi, Ahmed}},
	booktitle = {Cluster Comput},
	pages = {2017–2038},
	title = {DLBench: a comprehensive experimental evaluation of deep learning frameworks},
	doi = {https://doi.org/10.1007/s10586-021-03240-4},
	volume = {24},
	year = {2021}
}

@online{gtx-1070,
	author = {NVIDIA Corporation},
	title = {GeForce GTX 1070 Specifications},
	url = {https://www.nvidia.com/en-gb/geforce/graphics-cards/geforce-gtx-1070/specifications/},
	urldate = {2022-04-19}
}

@online{gpu-for-dl,
	author = {Run:AI},
	title = {Making the Most of GPUs for Your Deep Learning Project},
	url = {https://www.run.ai/guides/gpu-deep-learning#:~:text=Why%20Use%20GPUs%20for%20Deep,without%20sacrificing%20efficiency%20or%20power.},
	urldate = {2022-04-19}
}

@online{github-pytorch,
	author = {Facebook},
	title = {Tensors and Dynamic neural networks in Python with strong GPU acceleration},
	url = {https://github.com/pytorch/pytorch},
	urldate = {2022-04-19}
}

@online{github-tensorflow,
	author = {Google},
	title = {An Open Source Machine Learning Framework for Everyone},
	url = {https://github.com/tensorflow/tensorflow},
	urldate = {2022-04-19}
}

@online{cuda-doc,
	author = {NVIDIA Corporation},
	title = {CUDA Toolkit Documentation v11.6.2},
	url = {https://docs.nvidia.com/cuda/},
	urldate = {2022-04-19}
}

@online{cudnn-doc,
	author = {NVIDIA Corporation},
	title = {NVIDIA CUDNN DOCUMENTATION},
	url = {https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html},
	urldate = {2022-04-19}
}

@online{keras,
	author = {Google},
	title = {Keras},
	url = {https://keras.io/},
	urldate = {2022-04-19}
}

@misc{omalley2019kerastuner,
	title        = {KerasTuner},
	author       = {O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others},
	year         = 2019,
	howpublished = {\url{https://github.com/keras-team/keras-tuner}}
}

@online{auto-keras,
	title={Auto-Keras: An Efficient Neural Architecture Search System},
	author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
	booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages={1946--1956},
	year={2019},
	organization={ACM}
}

@online{pytorch-tensorboard-offical-documentation,
	author = {Facebook},
	title = {PyTorch: Visualizing Models, Data, And Training With Tensorboard},
	url = {https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html},
	urldate = {2022-04-20}
}

@online{img:cgan-vs-gan,
	author = {Aditya Sharma},
	title = {Image: Flow Diagram representing GAN and Conditional GAN},
	url = {https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/},
	urldate = {2022-04-20}
}


@online{adam-tutorial,
	author = {Artem Oppermann},
	title = {Optimierung in Deep Learning: AdaGrad, RMSProp, ADAM},
	date = {2021-08-10},
	url = {https://artemoppermann.com/de/optimierung-in-deep-learning-adagrad-rmsprop-adam/},
	urldate = {2022-04-24}
}

@misc{small-batch-size,
	doi = {10.48550/ARXIV.1804.07612},
	url = {https://arxiv.org/abs/1804.07612},
	author = {Masters, Dominic and Luschi, Carlo},
	title = {Revisiting Small Batch Training for Deep Neural Networks},
	publisher = {arXiv},
	year = {2018}
}

@misc{default-batch-size,
	doi = {10.48550/ARXIV.1206.5533},
	url = {https://arxiv.org/abs/1206.5533},
	author = {Bengio, Yoshua},
	title = {Practical recommendations for gradient-based training of deep architectures},
	publisher = {arXiv},
	year = {2012}
}

@article{fid,
	doi = {10.48550/ARXIV.1706.08500},
	url = {https://arxiv.org/abs/1706.08500},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	publisher = {arXiv},
	year = {2017},
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@online{example-one-sided-label-smoothing,
	author = {OpenAI},
	title = {image: Adversarial Example},
	date = {2021-08-10},
	url = {https://openai.com/content/images/2017/02/adversarial_img_1.png},
	urldate = {2022-04-24}
}


@article{JMLR:v15:srivastava14a,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	number  = {56},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@online{dense-layer,
	author = {Yugesh Verma},
	title = {A Complete Understanding of Dense Layers in Neural Networks},
	date = {2021-09-19},
	url = {https://analyticsindiamag.com/a-complete-understanding-of-dense-layers-in-neural-networks/},
	urldate = {2022-05-11}
}

@online{batch-normalization,
	author = {Martin Riva},
	title = {Batch Normalization in Convolutional Neural Networks},
	date = {2021-05-15},
	url = {https://www.baeldung.com/cs/batch-normalization-cnn},
	urldate = {2022-05-13}
}

@misc{batch-normalization-paper,
	doi = {10.48550/ARXIV.1502.03167},
	url = {https://arxiv.org/abs/1502.03167},	
	author = {Ioffe, Sergey and Szegedy, Christian},	
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	publisher = {arXiv},
	year = {2015}
}

@inproceedings{convolutional-malware,
	author = {Yakura, Hiromu and Shinozaki, Shinnosuke and Nishimura, Reon and Oyama, Yoshihiro and Sakuma, Jun},
	year = {2018},
	month = {03},
	pages = {127-134},
	title = {Malware Analysis of Imaged Binary Samples by Convolutional Neural Network with Attention Mechanism},
	doi = {10.1145/3176258.3176335}
}

@online{convolutional_transpose,
	author = {Kuan Wei},
	title = {Understand Transposed Convolutions},
	date = {2020-07-29},
	url = {https://towardsdatascience.com/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967},
	urldate = {2022-05-13}
}

@article{padding_stride,
	title={Dive into Deep Learning},
	author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
	journal={arXiv preprint arXiv:2106.11342},
	year={2021}
}

@online{convolutional_tut,
	author = {Jason Brownlee},
	title = {How Do Convolutional Layers Work in Deep Learning Neural Networks?},
	date = {2019-04-17},
	url = {https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/},
	urldate = {2022-05-13}
}


@online{label_encoding,
	author = {Ankush kunwar},
	title = {Categorical Encoding (Label / Ordinal / Integer encoding) in Feature engineering.},
	date = {2020-11-18},
	url = {https://medium.com/analytics-vidhya/categorical-encoding-label-ordinal-integer-encoding-in-feature-engineering-1beeaa00f0fa},
	urldate = {2022-05-13}
}

