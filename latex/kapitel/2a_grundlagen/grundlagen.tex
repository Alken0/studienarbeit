%!TEX root = ../../main.tex

\chapter{Grundlagen}
\todo{Quellen für alles raus suchen}

In diesem Kapitel werden Grundlagen zu neuronalen Netzen beschrieben.
Ziel ist es, einen einheitlichen Kenntnisstand voraussetzen zu können.
Das Kapitel enthält ausschließlich allgemeine Informationen und kann somit auch übersprungen werden.

\section{Schichten Neuronaler Netze}
Die Neuronen in neuronalen Netzen sind in Schichten angeordnet.
Diese Schichten können für unterschiedliche Zwecke unterschiedlich designt sein.
So können spezielle Aufgaben effizienter gelöst werden.
Eine Auswahl an Schichtenarten wird im Folgenden erläutert.

\subsection{Dense Layer}
Bei einem Dense Layer sind alle Neuronen mit jedem Ausgabewert der vorherigen Schicht verknüpft.
Das dicht vermaschte Netz kann aufgrund seiner vielen Parameter auch sehr viele Informationen extrahieren.
Die große Anzahl an Parametern bedeutet allerdings zugleich, dass es sehr ausgiebiges Training benötigt, um es korrekt anzulernen.
\newline

Zwar kann das Netz theoretisch universell eingesetzt werden, aufgrund des hohen Lernaufwandes wird das jedoch vermieden.
Stattdessen wird versucht, spezialisierter Schichten mit weniger Parameter und somit weniger Lernaufwand einzusetzen.
\newline

Der hohe Informationsgehalt der Schicht eignet sie sich allerdings gut bei Klassifikationsnetzen.
Dort wird sie meist am Ende eingesetzt, um zuvor die Parameter zu reduzieren.

\subsection{Convolutional Layer}
Ein Convolutional Layer wird oft in der Bildverarbeitung eingesetzt.
Die Schicht extrahiert Zusammenhänge in n-dimensionalen Eingaben bei benachbarten Werten.
Das wäre in einem 2d Bild zum Beispiel alle Pixel in der linken oberen Ecke.
Dafür wird ein Fenster über das Bild gelegt und systematisch verschoben, wobei jede Verschiebung einen neuen Ausgabewert generiert.
\newline

Vorteile der Schicht sind insbesondere die wenig benötigten Parameter.
Zudem wird das neuronale Netz mit der Schicht gezwungen, regionale Zusammenhänge statt die Daten als ganzes zu analysieren.
\todo{Bild einfügen aus BV-Vorlesung}

\subsection{Convolutional Transpose Layer}
Bei dem Convolutional Transpose Layer wird eine n-dimensionale Eingabe hoch skaliert.
Die Schicht funktioniert wie ihr Gegenstück, dem Convolutional Layer.
Allerdings wird die Richtung umgekehrt, sodass aus wenigen Daten viele gemacht werden.
Die Schicht nimmt dabei regionale Zusammenhänge an und überträgt sie auf die Ausgabe.


\begin{comment}
	
\section{Stichpunkte}
\subsection{GAN}
\paragraph{Anwendung}
\begin{itemize}
	\item Viel bei Generierungsaufgaben
	\item eignet sich immer bei Aufgaben, bei denen es keine klare Lösung gibt
\end{itemize}


\paragraph{Aufbau \cite{gan-original-paper, hyperparameters-gan-using-genetic-algorithm}}
\begin{itemize}
	\item 2 Neuronale Netze (Discriminator und Generator), die sich gegenseitig trainieren
	\item Generator: ist für die Bildgenerierung zuständig
	\item Discriminator: ist für die Bildverfizierung zuständig (echt oder fake)
	\item nach dem Training ist in der Regel nur noch der Generator interessant
	
	\item Training
	\begin{itemize}
		\item Generator generiert ein zufälliges Bild (kriegt dafür zufälligen Input)
		\item Discriminator: kriegt ein "echtes" oder ein generierte Bild vorgelegt -> muss entscheiden, ob das Bild "echt" ist
		\item Ziel Generator: Discriminator soll sich irren und sein generiertes Bild für echt halten
		\item Ziel Discriminator: richtige Entscheidung treffen
		\item dadurch, dass sich beide gegenseitig immer weiter verbessern, entstehen dann immer realistischere generierte Bilder
	\end{itemize}
	
	\item Aufbau
	\begin{itemize}
		\item TODO Bild dazu erstellen (die haben ein Beispiel zur Orientierung: \cite{hyperparameters-gan-using-genetic-algorithm})
		\item Discriminator wird einzeln trainiert
		\item um Generator zu trainieren, wird er mit Discriminator gekoppelt
	\end{itemize}
	
	\item mit Label
	\begin{itemize}
		\item Generator kriegt zusätzlich zum zufälligen Input noch das Label
		\item Discriminator kriegt neben dem Bild auch das zugehörige Label
		\item so kann dann später ein Label vorgegeben werden und dadurch ein spezielles zufälliges Bild generiert werden
	\end{itemize}
\end{itemize}

\subsection{Aktivierungsfunktionen}
\paragraph{Allgemein  \cite{activation-function}}
\begin{itemize}
	\item Output von Neuron verändern
\end{itemize}

\paragraph{Eigenschaften}
\begin{itemize}
	\item Nonlinear
	\item Range (z.b. 0-1)
	\item Continuously differentiable (immer weiter ableitbar, will man haben für lernen)
\end{itemize}

\paragraph{Beispiele}
\begin{itemize}
	\item Sigmoid (vanishing/exploding(?) gradient problem)
	\item tanh (besser als sigmoid, aber auch vanishing/exploding gradient problem)
	\item (leaky-/...) ReLu (besser als die davor, asymetrisch: $ReLu=max(0, x)$ )
	\item ELU (vielleicht für uns auch interessant? - wie leaky-Relu, bloss dass sich der 0 über $e^x - 1$ angenähert wird, wodurch der Übergang sanft ist; soll deutlich schneller und besser konvergiert)
\end{itemize}

\paragraph{LeakyReLu}
\begin{itemize}
	\item Vorteil: ableitbar und kein schlimmer Gradien descent
	\item Todo bild von Neuron mit einbringen
\end{itemize}


\subsection{Schichten}
\begin{itemize}
	\item Dense Layer
	\begin{itemize}
		\item jedes Neuron mit jeder Schicht davor verknüpft
		\item sehr viele Parameter
		\item sehr viel Information
		\item sehr allgemein, keine spezielle Informationsextraktion - kann theoretisch alles lernen (mit erhöhtem Aufwand)
		\item für uns am Ende des Discriminators zur Klassifikation interessant
	\end{itemize}
	\item Convolutional Layer
	\begin{itemize}
		\item downscaling (z.b. von Bildern)
		\item soll Zusammenhänge benachbarter Inputs erkennen
		\item relativ wenig zu lernende Parameter
		\item wie ein Feld, das langsam rüber wandert (Bild zur Erklärung suchen...)
		\item für uns im Discriminator interessant (am Anfang)
	\end{itemize}
	\item Convolutional Transpose Layer
	\begin{itemize}
		\item Gegenteil von Convolutional Layer -> upscaling
		\item im Generator interessant
		\item verhält sich ähnlich wie convolutional layer...
	\end{itemize}
\end{itemize}

\subsection{Hyperparameter}
\paragraph{Allgemein \cite{hyperparameters-search-in-machine-learning, hyperparameters-gan-using-genetic-algorithm}}
\begin{itemize}
	\item Parameter, die den Lernprozess definieren
	\item Gegensatz wären z.B. weights, die beim Training bestimmt werden
	\item schwierig festzulegen
	\item Beschreibung der zu untersuchenden Hyperparameter
	\item Auswirkung auf den Lernerfolg
	\item Was muss man dabei beachten
	\item Wie wird das gemessen?
	\begin{itemize}
		\item manuell (niemals optimal)
		\item Gridsearch (sehr rechenaufwändig)  \cite{hyperparameters-grid-search}
		\item Random search
		\item Bayesian Optimization
	\end{itemize}
\end{itemize}

\paragraph{Einfache Beispiele \cite{hyperparameters-gan-using-genetic-algorithm, hyperparameters-what-how}}
\begin{itemize}
	\item Lernrate
	\item Dropout
	\item Activation Function (+Momentum/...)
	\item Batch size (good default: 32 -> die gehen eher hoch 32,64,128,...)
	\item Anzahl Epochen
	\item Number of layers
	\item number of units in dense layer
	\item loss function
	\item Generator/Discriminator optimizer
\end{itemize}

\paragraph{Herausforderungen \cite{hyperparameters-search-in-machine-learning}}
\begin{itemize}
	\item Rechenaufwand
	\item Zufall im Training
	\item Stark unterschiedliche Auswirkung auf das Training für verschiedene Hyperparameter
\end{itemize}

\subsection{Lossfunction}
\paragraph{Allgemein \cite{lossfunction-opportunities-and-challenges, common-loss, russel-norvig}}
\begin{itemize}
	\item Allgemein beschreibt eine Loss-Funktion den Unterschied zwischen Erwartungswert und Ergebnis
	\item der Wert kann dann für das Training des Neuronalen Netzes verwendet werden
	\item gibt verschiedene Ansätze zur Berechnung
\end{itemize}

\paragraph{Einfache Beispiele (wobei $y$ = Erwartungswert, $\hat{y}$ = Ergebnis) \cite{russel-norvig}}
\begin{itemize}
	\item Absolute Value Loss: $L_1(y, \hat{y}) = |y - \hat{y}|$
	\item Squared Error Loss: $L_2(y, \hat{y}) = (y - \hat{y})^2$
\end{itemize}

\paragraph{Herausforderungen \cite[p. 710]{russel-norvig}}
\begin{itemize}
	\item \textit{Noise} 
	Einzelne Datensätze spiegeln nicht den Optimalfall wieder
	
	\item \textit{Small Scale Learning} 
	nur sehr begrenztes Trainingsset, wodurch das Optimum eventuell gar nicht durch die Daten abgebildet werden kann
	
	\item \textit{Large Scale Learning} 
	zu viele Daten, wird zum Rechenproblem -> Ergebnis wird approximiert, wir haben sehr begrenzte Rechenleistung...
\end{itemize}



\subsection{Learning Rate}
\paragraph{Allgemein \cite{learningrate-how-to-configure}}
\begin{itemize}
	\item Legt fest, wie stark sich das Neuronale Netzwerk nach jeder Lerniteration verändern soll.
	\item Beeinflusst die Geschwindigkeit, in der das Neuronale Netzwerk lernt
	\item lässt sich nur über trial and error bestimmen, aber normalerweise zwischen 1 und $10^{-6}$ => default bei etwa 0.01
	\item kleinere Batch-Sizes sind besser bei kleineren Lernraten
	\textit{'Further, smaller batch sizes are better suited to smaller learning rates given the noisy estimate of the error gradient.' \cite{learningrate-how-to-configure}}
\end{itemize}

\paragraph{Arten \cite{learningrate-understanding}}
\begin{itemize}
	\item \textit{Fester Wert} 
	siehe Beispiele
	
	\item \textit{Decay}
	verhindert, dass über 'Täler' hinübergesprungen wird
	
	\item \textit{Momentum}
	falls sich das Neuronale Netz lange in die gleiche Richtung entwickelt wird die Lernrate erhöht. (Analogie: Ball der einen Berg hinunter rollt und dabei kleinere Täler überspringt)
	
	\item \textit{Step Based}
	wird anhand des steps verändert (sinnloses Beispiel: $LearningRate = Step/100$)
	
	\item \textit{Adaptiv}
	zum Beispiel $Adam$ (nutzen wir) \cite{adam}
	
	\begin{itemize}
		\item suggested as default optimization
		\item Normalerweise wird eine Lernrate auf alle Weights angewendet
		\item bei Adam gibt es: ProParameterLearningrate und MomentumProParameter
	\end{itemize}
\end{itemize}


\subsection{Num Units}
\paragraph{Allgemein \cite{nodes-how-to-configure}}
\begin{itemize}
	\item unit = ein Neuron in einem dense-layer
	\item dense layer kann beliebig viele Neuronen haben, ohne die Netzarchitektur zu brechen
	\item folglich kann das frei gewählt werden
	\item mehr units => viel mehr zu trainierende Parameter => viel mehr Rechenkapazität
	\item mehr units => bessere Generalisierung =>? bessere Ergebnisse
	\item Paper von 2009: 2n+1 und 2 hidden layer (n=Anzahl input-neuronen) als default? -> könnten wir auch ausprobieren/Orientierung nehmen? (wobei input dann das runtergescalete sein müsste und nicht die original Bilder, sonst wären das sehr sehr viele Neuronen...) \cite{nodes-hiddenlayers-how-many}
\end{itemize}

\paragraph{Arten (der Bestimmung optimaler Node-Anzahl) \cite{nodes-how-to-configure, nodes-hiddenlayers-how-many}}
\begin{itemize}
	\item Random
	\item Grid -> unsere Variante
	\item Heuristic
	\item Exhaustive
\end{itemize}

\subsection{Hyperparameter Analyse}


\section{Noch Interessant}
\subsection{Wasserstein GAN}
Verbesserung bei mehreren Probleme von GANs (Uninformative Loss, Mode Collapse, Unstable Training).
Nicht allzu schwer nachzuimplementieren, aber nochmal genau informieren?

content...
\end{comment}
