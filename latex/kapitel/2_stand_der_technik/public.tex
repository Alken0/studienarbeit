\chapter{Stand der Technik}

SEHR GUTE ARBEIT ZUM ORIENTIEREN (und kommt auch mit einem extrem viel kleineren netz aus...) \cite{gan-conditional}

\section{Allgemein GAN}

\paragraph{Anwendung}
\begin{itemize}
	\item Viel bei Generierungsaufgaben
	\item eignet sich immer bei Aufgaben, bei denen es keine klare Lösung gibt
\end{itemize}


\paragraph{Aufbau \cite{gan-original-paper, hyperparameters-gan-using-genetic-algorithm}}
\begin{itemize}
	\item 2 Neuronale Netze (Discriminator und Generator), die sich gegenseitig trainieren
	\item Generator: ist für die Bildgenerierung zuständig
	\item Discriminator: ist für die Bildverfizierung zuständig (echt oder fake)
	\item nach dem Training ist in der Regel nur noch der Generator interessant
	
	\item Training
	\begin{itemize}
		\item Generator generiert ein zufälliges Bild (kriegt dafür zufälligen Input)
		\item Discriminator: kriegt ein "echtes" oder ein generierte Bild vorgelegt -> muss entscheiden, ob das Bild "echt" ist
		\item Ziel Generator: Discriminator soll sich irren und sein generiertes Bild für echt halten
		\item Ziel Discriminator: richtige Entscheidung treffen
		\item dadurch, dass sich beide gegenseitig immer weiter verbessern, entstehen dann immer realistischere generierte Bilder
	\end{itemize}
	
	\item Aufbau
	\begin{itemize}
		\item TODO Bild dazu erstellen (die haben ein Beispiel zur Orientierung: \cite{hyperparameters-gan-using-genetic-algorithm})
		\item Discriminator wird einzeln trainiert
		\item um Generator zu trainieren, wird er mit Discriminator gekoppelt
	\end{itemize}
	
	\item mit Label
	\begin{itemize}
		\item Generator kriegt zusätzlich zum zufälligen Input noch das Label
		\item Discriminator kriegt neben dem Bild auch das zugehörige Label
		\item so kann dann später ein Label vorgegeben werden und dadurch ein spezielles zufälliges Bild generiert werden
	\end{itemize}
\end{itemize}

\section{Hyperparameter}
Beschreibung der zu untersuchenden Hyperparameter (Auswirkung auf den Lernerfolg/Was muss man dabei beachten/Wie wird das gemessen?/....)

\subsection{Hyperparameter}
\paragraph{Allgemein \cite{hyperparameters-search-in-machine-learning, hyperparameters-gan-using-genetic-algorithm}}
\begin{itemize}
	\item Parameter, die den Lernprozess definieren
	\item Gegensatz wären z.B. weights, die beim Training bestimmt werden
	\item schwierig festzulegen
	\begin{itemize}
		\item manuell (niemals optimal)
		\item Gridsearch (sehr rechenaufwändig)  \cite{hyperparameters-grid-search}
		\item Random search
		\item Bayesian Optimization
	\end{itemize}
\end{itemize}

\paragraph{Einfache Beispiele \cite{hyperparameters-gan-using-genetic-algorithm, hyperparameters-what-how}}
\begin{itemize}
	\item Lernrate
	\item Dropout
	\item Activation Function (+Momentum/...)
	\item Batch size (good default: 32 -> die gehen eher hoch 32,64,128,...)
	\item Anzahl Epochen
	\item Number of layers
	\item number of units in dense layer
	\item loss function
	\item Generator/Discriminator optimizer
\end{itemize}

\paragraph{Herausforderungen \cite{hyperparameters-search-in-machine-learning}}
\begin{itemize}
	\item Rechenaufwand
	\item Zufall im Training
	\item Stark unterschiedliche Auswirkung auf das Training für verschiedene Hyperparameter
\end{itemize}

\subsection{Lossfunction}
\paragraph{Allgemein \cite{lossfunction-opportunities-and-challenges, common-loss, russel-norvig}}
\begin{itemize}
	\item Allgemein beschreibt eine Loss-Funktion den Unterschied zwischen Erwartungswert und Ergebnis
	\item der Wert kann dann für das Training des Neuronalen Netzes verwendet werden
	\item gibt verschiedene Ansätze zur Berechnung
\end{itemize}

\paragraph{Einfache Beispiele (wobei $y$ = Erwartungswert, $\hat{y}$ = Ergebnis) \cite{russel-norvig}}
\begin{itemize}
	\item Absolute Value Loss: $L_1(y, \hat{y}) = |y - \hat{y}|$
	\item Squared Error Loss: $L_2(y, \hat{y}) = (y - \hat{y})^2$
\end{itemize}

\paragraph{Herausforderungen \cite[p. 710]{russel-norvig}}
\begin{itemize}
	\item \textit{Noise} 
	Einzelne Datensätze spiegeln nicht den Optimalfall wieder
	
	\item \textit{Small Scale Learning} 
	nur sehr begrenztes Trainingsset, wodurch das Optimum eventuell gar nicht durch die Daten abgebildet werden kann
	
	\item \textit{Large Scale Learning} 
	zu viele Daten, wird zum Rechenproblem -> Ergebnis wird approximiert, wir haben sehr begrenzte Rechenleistung...
\end{itemize}



\subsection{Learning Rate}
\paragraph{Allgemein \cite{learningrate-how-to-configure}}
\begin{itemize}
	\item Legt fest, wie stark sich das Neuronale Netzwerk nach jeder Lerniteration verändern soll.
	\item Beeinflusst die Geschwindigkeit, in der das Neuronale Netzwerk lernt
	\item lässt sich nur über trial and error bestimmen, aber normalerweise zwischen 1 und $10^{-6}$ => default bei etwa 0.01
	\item kleinere Batch-Sizes sind besser bei kleineren Lernraten
	\textit{'Further, smaller batch sizes are better suited to smaller learning rates given the noisy estimate of the error gradient.' \cite{learningrate-how-to-configure}}
\end{itemize}

\paragraph{Arten \cite{learningrate-understanding}}
\begin{itemize}
	\item \textit{Fester Wert} 
	siehe Beispiele
	
	\item \textit{Decay}
	verhindert, dass über 'Täler' hinübergesprungen wird
	
	\item \textit{Momentum}
	falls sich das Neuronale Netz lange in die gleiche Richtung entwickelt wird die Lernrate erhöht. (Analogie: Ball der einen Berg hinunter rollt und dabei kleinere Täler überspringt)
	
	\item \textit{Step Based}
	wird anhand des steps verändert (sinnloses Beispiel: $LearningRate = Step/100$)
	
	\item \textit{Adaptiv}
	zum Beispiel $Adam$ (nutzen wir) \cite{adam}
	
	\begin{itemize}
		\item suggested as default optimization
		\item Normalerweise wird eine Lernrate auf alle Weights angewendet
		\item bei Adam gibt es: ProParameterLearningrate und MomentumProParameter
	\end{itemize}
\end{itemize}


\subsection{Num Units}
\paragraph{Allgemein \cite{nodes-how-to-configure}}
\begin{itemize}
	\item unit = ein Neuron in einem dense-layer
	\item dense layer kann beliebig viele Neuronen haben, ohne die Netzarchitektur zu brechen
	\item folglich kann das frei gewählt werden
	\item mehr units => viel mehr zu trainierende Parameter => viel mehr Rechenkapazität
	\item mehr units => bessere Generalisierung =>? bessere Ergebnisse
	\item Paper von 2009: 2n+1 und 2 hidden layer (n=Anzahl input-neuronen) als default? -> könnten wir auch ausprobieren/Orientierung nehmen? (wobei input dann das runtergescalete sein müsste und nicht die original Bilder, sonst wären das sehr sehr viele Neuronen...) \cite{nodes-hiddenlayers-how-many}
\end{itemize}

\paragraph{Arten (der Bestimmung optimaler Node-Anzahl) \cite{nodes-how-to-configure, nodes-hiddenlayers-how-many}}
\begin{itemize}
	\item Random
	\item Grid -> unsere Variante
	\item Heuristic
	\item Exhaustive
\end{itemize}


\section{Noch Interessant}
\subsection{Wasserstein GAN}
Verbesserung bei mehreren Probleme von GANs (Uninformative Loss, Mode Collapse, Unstable Training).
Nicht allzu schwer nachzuimplementieren, aber nochmal genau informieren?
