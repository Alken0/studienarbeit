% !TEX root = ../../main.tex
% !TeX spellcheck = de_DE

\chapter{Methodik}
In diesem Kapitel werden zunächst Eigenschaften der Trainingsdaten erläutert.
Dabei wird auf die Bildeigenschaften und Besonderheiten der dargestellten Figuren in den Bildern eingegangen.
Danach erfolgt eine Festlegung der Kriterien für ein erfolgreiche trainiertes 'GAN', das Grundlage für spätere Vergleiche von Trainingserfolgen sein wird.

\section{Trainingsdaten}
\todo[inline, shadow]{Rechenkapazität mit einbringen $\rightarrow$ Kompromiss von der Größe oder durch MNIST begründen?}
Das Dataset wurde im Rahmen der Studienarbeit generiert und besteht für das Training aus 1500 Bildern.
Die 1500 Bilder teilen sich dabei in jeweils 500 Bilder pro Formklasse (Dreieck, Kreis, Quadrat) auf.

\subsection{Generierung}
Bei den Trainingsdaten handelt es sich um synthetisch generierte Bilder mit geometrischen Figuren. 
Zwar gibt es bereits Datensätze mit ähnlichen Bildern \cite{dataset:2d-geometric-shapes-dataset, dataset:four-shapes}, im Rahmen der Studienarbeit werden jedoch keine vorgefertigten Datensätze verwendet. 
Denn die Generierung eigener Bilder erlaubt eine größere Kontrolle über Eigenschaften der Bilder und Inhalte, als vorgefertigte Sets.
Damit trotzdem eine Vergleichbarkeit zu anderen Arbeiten gewährleistet werden kann, wird die Generierung deterministisch reproduzierbar und dokumentiert sein.

\subsection{Eigenschaften}
Alle Bilder haben eine Größe von 28x28 Pixel und sind Schwarz-Weiß kodiert.
Der Grauwert jedes Pixels wird durch einen Channel angegeben, dessen Werte durch eine Zahl im ganzzahligen Intervall $[0; 255]$ festgelegt ist.
Dabei entspricht die Zahl 0 einem schwarzen Pixel und die Zahl 255 repräsentiert einen weißen Pixel.
\newline

Auf jedem Bild ist eine geometrische Form abgebildet.
Bei den geometrischen Formen handelt es sich um gleichseitige Dreiecke, Quadrate und Kreise.
Die Formen unterscheiden sich in Größe und Position auf dem Bild.
Andere Transformationen, wie beispielsweise Rotationen, werden nicht angewendet.
\newline

Die Quadrate und gleichseitigen Dreiecke sind zwischen 5 und 16 Pixel groß.
Die Größe bezieht sich dabei auf die Seitenlänge der Seiten.
Die Kreise sind zwischen 5 und 10 Pixel groß.
Hierbei gibt der Wert den Radius des Kreises an.
\newline

Die Formen sind so im Bild positioniert, dass sie Seitenränder touchieren können.
Allerdings ist eine Form immer vollständig im Bild abgebildet und wird vom Bildrand nicht geschnitten.

\newcommand{\trainDataImage}[1]{\subfloat{\fbox{\includegraphics{#1}}}}
\begin{figure}[H]
	\centering
	\trainDataImage{kapitel/3\_methodik/data/circle\_00.png}
	\trainDataImage{kapitel/3\_methodik/data/circle\_01.png}
	\trainDataImage{kapitel/3\_methodik/data/circle\_02.png}\par 
	
	\trainDataImage{kapitel/3\_methodik/data/rectangle\_00.png}
	\trainDataImage{kapitel/3\_methodik/data/rectangle\_01.png}
	\trainDataImage{kapitel/3\_methodik/data/rectangle\_02.png}\par 
	
	\trainDataImage{kapitel/3\_methodik/data/triangle\_00.png}
	\trainDataImage{kapitel/3\_methodik/data/triangle\_01.png}
	\trainDataImage{kapitel/3\_methodik/data/triangle\_02.png}\par 
	
	\caption{Auswahl an zufällig generierten Trainingsbilder}
\end{figure}

\section{Architekturen}
\subsection{API}
Um alle möglichen Architekturen in das Training einbinden zu können müssen sie die gleichen Ein- und Ausgaben besitzen.
Die Anforderungen an die Ein- und Ausgabe wie folgt definiert.
\newline

Der Generator erhält als Eingabe einen 100 Stellen langes Array mit Zufallszahlen (Latent-Dimension) und eine Zahl zwischen 0 und 2, die das Label repräsentiert.
Die Ausgabe des Generators ist ein Array mit den gleichen Dimensionen, wie ein Bild (28x28x1).
\newline

Der Discriminator erhält als Eingabe ein Bild als Array und zusätzlich ein Label als Zahl.
Die Ausgabe ist eine Zahl zwischen 1 und -1, die die vorhergesagte Echtheit angibt.

\section{Verfahren zur Hyperparametersuche}
\todo[shadow,inline]{Aufbau ändern in: Kriterien $\rightarrow$ Tabelle $\rightarrow$ Auswahl/Bewertung (siehe Kommentare)}
Zur Auswahl von Hyperparametern gibt es verschiedene Verfahren.
Dazu gehören unter anderem die Manuelle Suche, Gridsearch, Random Search und Genetic Algorithms (siehe \cref{chapter:verfahren-bestimmung-hyperparameter}).
In diesem Abschnitt werden die verschiedenen Verfahren verglichen und zusätzliche Anpassungen erläutert.

\subsection{Kriterien}
\todo[shadow, inline]{Implementierungsaufwand, Laufzeit, Komplexität, Unterstützung durch Tensorflow, ?}

\subsection{Vergleich}
In der Studienarbeit wird ausschließlich die Gridsearch zur Bestimmung der Hyperparameter zur Anwendung kommen.
Die Entscheidung ist mit den nachfolgenden Punkten begründet.

\paragraphNewLine{Manuelle Suche}
Die ausschließlich manuelle Suche ist zwar sehr flexibel\todo{anderes wort}, aber zu aufwändig.
Eine weitreichende Suche mit ständiger Neuevaluierung der Trainingsergebnisse ist für den kurzen Zeitraum der Studienarbeit nicht verhältnismäßig.
Zudem ist das Verfahren für eine komplexe Hyperparametersuche nicht mehr zeitgemäß.
Die anderen vorgestellten Verfahren sind der manuellen Suche immer überlegen.

\paragraphNewLine{Zufallssuche}
Die Zufallssuche ist der Gridsearch sehr ähnlich.
Statt festgelegten Zahlen werden Zufallszahlen verwendet.
Dadurch wird ein größeres Spektrum an Kombinationen abgedeckt.
Allerdings führt das Verfahren in der Praxis nicht unbedingt zu besseren Ergebnissen als die Gridsearch.
Zusätzlich führt es zu einem größeren Implementierungsaufwand und wieder mehr Hyperparametern als die Gridsearch.
Der erhöhte Implementierungsaufwand ist durch eine zusätzliche Zufallsfunktion zu begründen.
Die Funktion wird benötigt, um die alle Kombinationen anzupassen.
Im Vergleich dazu kann bei der Gridsearch pro Hyperparameter ein Array mit verschiedenen Werten übergeben werden.
Die zusätzlichen Hyperparameter hängen auch mit der Zufallsfunktion zusammen.
Denn der zufällige Abstand zum Originalwert muss in einem sinnvollen Intervall liegen.
Die Bestimmung der Grenzen des Intervalls benötigt zusätzliche Anpassungen und Training.

\paragraphNewLine{Evolutionäre Suche}
Die evolutionäre Suche ist sowohl mit einem hohen Implementierungs-, als auch Rechenaufwand verbunden.
Dafür ist es möglich, sehr viele Hyperparameter auf ihre Zusammenhänge zu untersuchen.
Jedoch reduzieren wir die Hyperparameter auf eine kleine Auswahl, weswegen der Aufwand nicht gerechtfertigt ist.

\paragraphNewLine{Gridsearch}
Die Gridsearch ist für die Arbeit am besten geeignet, da sie zum einen einen sehr geringen Implementierungsaufwand mit sich bringt.
Es müssen nur die Hyperparameter festgelegt werden, der Suchprozess kann dann mithilfe von Tensorboard grafisch veranschaulicht werden.

\subsection{Bewertung}
\todo[shadow, inline]{vorher: Anpassung an Gridsearch $\rightarrow$ markus: gehört in Bewertung/Auswahl $\rightarrow$ umschreiben}
Zwar findet die Gridsearch in keiner intelligenten Weise die optimalen Hyperparameter, aber durch kurze Trainings- und Validierungsintervalle können schlecht trainierende Kombinationen schnell aussortiert werden.
Dafür können am Anfang $n$ GANs auf $m$ Generationen trainiert werden.
Nach dem Training werden die Ergebnisse evaluiert und die Hälfte der GANs aussortiert 
Im Gegenzug wird die Anzahl der zu trainierenden Generationen verdoppelt.
So bleibt der Aufwand für das Training, bei deutlich erhöhter Intensität, der gleiche.
Es ist anzumerken, dass die neuronalen Netze weitertrainiert werden und der vorhandene Lernfortschritt nicht zurückgesetzt wird.
Der Prozess wird beliebig oft wiederholt und es können zu jedem Zeitpunkt Zusammenhänge zwischen den verbliebenen GANs analysiert werden.

\section{Training}
Im Trainingsprozess werden verschiedene Architekturen mit unterschiedlichen Hyperparametern getestet.
Die ausgewählten Architekturen orientieren sich an bereits existierender Forschung und Tutorials (siehe \cref{chapter:related-work}).
Insbesondere aufgrund der Label-Abhängigkeit (im Rahmen der Studienarbeit entsteht ein Conditional-GAN) werden jedoch Anpassungen erfolgen.


\subsection{Auswahl zu optimierender Hyperparameter}
\todo[inline, shadow]{überarbeiten}
Es ist nicht möglich, alle möglichen Hyperparameter zu optimieren.
Eine solche Optimierung benötigt zu viele Ressourcen, die im Rahmen der Studienarbeit nicht vorhanden sind.
Deshalb wird die Optimierung auf einige vielversprechende Hyperparameter beschränkt.
\newline

Die auszuwählenden Hyperparameter sollen dabei zum einen eine vergleichsweise hohe Auswirkung auf den Trainingserfolg haben.
Zum anderen sollen sie sehr allgemein sein, um sie gleichermaßen auf die verschiedenen Architekturen anwenden zu können.
Das garantiert eine bessere Vergleichbarkeit zwischen den Architekturen.
\newline

Die Hyperparameter \textit{Batch-Size}, \textit{Learning-Rate Generator}, \textit{Learning-Rate Discriminator}, \textit{Dropout} und \textit{Smoothness} entsprechen diesen Voraussetzungen.
\newline

Die Learning-Rate hat allgemein einen sehr hohen Einfluss und sollte immer angepasst werden \cite{learning-rate-most-important}.
Ähnlich wie die Learning-Rate ist auch die Batch-Size ein sehr beliebter Hyperparameter für Anpassungen und wird in unterschiedlichen Tutorials angegeben \cite{tutorial:tune-batch-size-analyticsvidhya, tutorial:tune-batch-size-machinelearningmastery} \todo{Begründung aus der quelle übernehmen? \cite{tutorial:tune-batch-size-machinelearningmastery}}.
Sowohl Dropout als auch Smoothness sind vor allem für einen persönlichen Erkenntnisgewinn mit aufgenommen worden.
Die Smoothness entstammt einer Idee von Ian Goodfellow \cite{ian-goodfellow-onesided-label-smoothing}.

\subsection{Trainingsablauf}
Vor der Implementierung und Analyse der verschiedenen Architekturen werden zunächst die Trainingsdaten generiert.
Danach erfolgt die Implementierung einer neuen Architektur und Festlegung der Hyperparameter.
Schließlich wird das Training durchgeführt.
Beim Training werden automatisch alle Hyperparameter-Kombinationen ausprobiert, Log-Dateien erstellt und auch die FID-Werte für die einzelnen Durchgänge berechnet.
Alle Metriken in den Logs werden dabei auf Testdaten angewendet, um ein Overfitting erkennen zu können.
Die Logs können im Anschluss mithilfe von Tensorboard visualisiert werden.

\section{Bewertung}
\subsection{Erfolgskriterien}
Damit die Konfigurationen treffend vorausgewählt werden könne, müssen die Kriterien für ein erfolgreich trainiertes GAN klar definiert werden.
Als Erfolgskriterien zählen in diesem Fall die Varietät der generierten Bilder und die Korrektheit der generierten Figuren.
Beide Kriterien werden im Folgenden noch einmal genauer erläutert.

\paragraphNewLine{Varietät}
Die Varietät bezieht zum einen auf die Ähnlichkeit zwischen Trainingsbildern und den generierten Bildern der GANs.
Die Ähnlichkeit zwischen diesen beiden Bildersets beschreibt, wie gut das GAN 'etwas neues schaffen' kann, oder ob es nur Trainingsbilder dupliziert.
Sollte die Ähnlichkeit sehr gering sein, werden viele 'neue' Bilder generiert, was sehr positiv zu bewerten ist.
Zudem bezieht sich die Varietät auf die generierten Bilder untereinander.
Sie sollten auch möglichst verschieden sein, was oftmals nicht der Fall ist.
Das Phänomen ist als 'mode-collapse' bekannt (siehe Stand der Technik). %TODO steht das auch in Stand der Technik?

Beide Probleme werden mittels einer manuellen Analyse ausgewertet.
% https://scikit-image.org/docs/0.12.x/api/skimage.measure.html#skimage.measure.compare_ssim

\paragraphNewLine{Korrektheit}
Neben der Varietät besitzt auch die Korrektheit eine hohe Bedeutung für die Bewertung der GANs.
Dabei muss sichergestellt werden, dass die richtige Figur generiert wurde und erkennbar ist.
Die Figuren müssen außerdem den gleichen Kriterien wie die Trainingsdaten genügen, das heißt, die Figuren sollten zum Beispiel vollständig innerhalb des Bildes abgebildet sein.
\todo{markus: wertung}
Es ist allerdings eher unwahrscheinlich, dass das GAN Bilder generiert, die keinem Pendant aus den Trainingsbildern entsprechen.
Ein weiterer wichtiger Aspekt der Bilder ist das Verhalten im Hintergrund der Figur.
So sollten im Hintergrund möglichst keine anderen Figuren oder Pixelfragmente erzeugt werden.

All diese Kriterien werden durch eine manuelle Überprüfung evaluiert werden.

\begin{itemize}
	\item manuelle Auswertung
	\item Frechet Inception Distance \cite{frechet-inception-distance}
\end{itemize}

\subsection{Logs}
Während des Trainings wird eine Reihe von Logs zur Analyse erzeugt.
Die Logs werden bei der Erzeugung automatisch in Architektur und Hyperparameterkombination unterteilt.
Durch Unterteilung ist die Grundlage einer fundierten Auswahl einer Architektur mit den besten Ergebnissen sichergestellt.
\newline

Während dem Training werden mehrere Werte geloggt.
Ein Wert ist dabei der Loss, sowohl für den Generator, als auch für den Discriminator.
Der Loss wird dabei für jede Epoche geloggt und ist vor allem zur Analyse des Trainingsprozesses interessant.
So können nicht funktionierende Architekturen schon früher abgebrochen werden und verbessert werden.
\newline

Für die Auswertung sind vor allem die generierten Bilder und der FID-Index relevant.
Die Bilder werden nach jeder Epoche mit einer immer identischen Latent-Dim erzeugt.
Dadurch ist eine bessere Vergleichbarkeit gewährleistet.
Tensorboard erlaubt dann eine Ansicht mit Zeitachse, um den Lernprozess über die verschiedenen Epochen beobachten zu können.
\newline

Der FID-Index ist sehr aufwändig zu berechnen.
Deshalb wird er erst am Ende mit den finalen Bildern erzeugt.
Bei einem Overfitting könnten so theoretisch schlechtere Ergebnisse für ein Modell entstehen.
Durch eine zusätzliche manuelle Analyse über die einzelnen Epochen und der vergleichsweise geringen Epochenanzahl ist das Risiko einer potentiellen Verfälschung der Ergebnisse jedoch sehr gering.
\newline

Die gesammelten Daten sind dabei alle nach Architektur und Hyperparameterkombination unterteilt.
Die Unterteilung erlaubt die Auswahl einer möglichst optimalen Kombination aus beidem.


\begin{comment}

Die Korrektheit bezieht sich dabei auf die generierte Figur im Bild oder auch umliegende Bildpunkte.
Für die Korrektur der richtigen Formen gibt es mehrere Möglichkeiten:
\begin{description}
	\item[Neuronales Kontrollnetz]
	Dafür wird ein weiteres Netzwerk zur Bewertung der Resultate trainiert.
	Diese Variante ist sehr ungenau, da sie wieder von dem Trainingserfolg eines Neuronalen Netzes abhängt.
	Allerdings erlaubt das Neuronale Netzwerk die Analyse von großen Datensätzen, die aber in diesem Fall nicht in der Form nötig sein wird.
	
	\item[händisch]
	Das händische Kontrollieren ist sehr aufwändig.
	Für die Kontrolle sollte das aber möglich sein.
	
	\item[Formvergleich]
	Es ist möglich, die Lösung zu brute-forcen.
	Das bedeutet, es wird jede mögliche Form über das Bild gelegt und diejenige mit der höchsten Überschneidung ausgewählt und als Indikator genommen.
\end{description}



\section{GAN: Bewertungs- und Vergleichskriterien}

% How to write methods section: http://rc.rcjournal.com/content/respcare/49/10/1229.full.pdf
\subsection{Warum synthetische Daten?}
Mehr Kontrolle über die Bilder:
\footnote{Die meisten Anpassungen erlauben auch Rechenleistung zu reduzieren, falls Computer überlastet sein sollten.}
\begin{enumerate}
	\item Anpassung von Größe
	\item Anzahl der unterschiedlichen labels (wieviel unterschiedliche Formen sind enthalten)
	\item sich unterscheidende Features in den Bildern (wo genau sind die Bilder zu finden, unterschiedliche Größe, Rotation?)
	\item wie generiert? $\rightarrow$ Deterministisch $\rightarrow$ Seed
	\item weniger Arbeitsaufwand als händische Generierung
\end{enumerate}

\subsection{Welche Eigenschaften haben die Bilder genau?}
\begin{enumerate}
	\item größe: 28x28
	\item farbe: 1-dimensional $\rightarrow$ scharz-weiß 
\end{enumerate}

\section{Training}
Ziel: GAN das möglichst diverse und korrekte Bilder erzeugt $\rightarrow$ Wie wird das erreicht?
\begin{enumerate}
	\item Anpassung von Hyperparametern (Learning-rate, batch-size, momentum)
	\footnote{https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a}
	\footnote{Grid-search (erlaubt methodisches Suchen nach optimalen Hyperparametern): https://www.tensorflow.org/tensorboard/hyperparameter\_tuning\_with\_hparams}
	\item Anpassung von Neuralen Netzarchitektur
	\footnote{https://lab.wallarm.com/the-first-step-by-step-guide-for-implementing-neural-architecture-search-with-reinforcement-learning-using-tensorflow-99ade71b3d28/}
\end{enumerate}

\section{Bewertung bzw. Vergleich der GANs}
Ziel: messbare Vergleichskriterien

\subsection{Korrektheit der Bilder}
\begin{enumerate}
	\item richtige Form für das angegebene Label? 
	\item muss per Hand bestimmt werden? oder festen algorithmus, der form erkennt? neuronales netz wäre zu ungenau?
\end{enumerate}

\subsection{Diversität der generierten Bilder}
\begin{enumerate}
	\item Vergleich zu Trainingsdaten (wird etwas neues geschaffen?)
	\item Vergleich zu anderen generierten Bildern $\rightarrow$ (Parital) Mode-Collapse?
	\footnote{https://developers.google.com/machine-learning/gan/problems}
	\footnote{eventuell 2 discriminator?: https://dl.acm.org/doi/10.1145/3283254.3283282}
	\footnote{Google: https://research.google/pubs/pub45829/}
	
	\item Vergleich von Bildern durch 'Pixel by Pixel' Ähnlichkeit bestimmen $\rightarrow$ die X bilder mit einer Ähnlichkeit über Y \% können dann angeguckt werden
\end{enumerate}
\end{comment}