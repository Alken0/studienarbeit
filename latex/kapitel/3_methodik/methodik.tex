% !TEX root = ../../main.tex
% !TeX spellcheck = de_DE

\chapter{Methodik}
In diesem Kapitel werden zunächst Eigenschaften der Trainingsdaten erläutert.
Dabei wird auf die Bildeigenschaften und Besonderheiten der dargestellten Figuren in den Bildern eingegangen.
Danach erfolgt eine Festlegung der Kriterien für ein erfolgreiche trainiertes 'GAN', das Grundlage für spätere Vergleiche von Trainingserfolgen sein wird.

\section{Trainingsdaten}
\todo[inline, shadow]{Rechenkapazität mit einbringen $\rightarrow$ Kompromiss von der Größe oder durch MNIST begründen?}
Das Dataset wurde im Rahmen der Studienarbeit generiert und besteht für das Training aus 1500 Bildern.
Die 1500 Bilder teilen sich dabei in jeweils 500 Bilder pro Formklasse (Dreieck, Kreis, Quadrat) auf.

\subsection{Generierung}
Bei den Trainingsdaten handelt es sich um synthetisch generierte Bilder mit geometrischen Figuren. 
Zwar gibt es bereits Datensätze mit ähnlichen Bildern \cite{dataset:2d-geometric-shapes-dataset, dataset:four-shapes}, im Rahmen der Studienarbeit werden jedoch keine vorgefertigten Datensätze verwendet. 
Denn die Generierung eigener Bilder erlaubt eine größere Kontrolle über Eigenschaften der Bilder und Inhalte, als vorgefertigte Sets.
Damit trotzdem eine Vergleichbarkeit zu anderen Arbeiten gewährleistet werden kann, wird die Generierung deterministisch reproduzierbar und dokumentiert sein.

\subsection{Eigenschaften}
Alle Bilder haben eine Größe von 28x28 Pixel und sind Schwarz-Weiß kodiert.
Der Grauwert jedes Pixels wird durch einen Channel angegeben, dessen Werte durch eine Zahl im ganzzahligen Intervall $[0; 255]$ festgelegt ist.
Dabei entspricht die Zahl 0 einem schwarzen Pixel und die Zahl 255 repräsentiert einen weißen Pixel.
\newline

Auf jedem Bild ist eine geometrische Form abgebildet.
Bei den geometrischen Formen handelt es sich um gleichseitige Dreiecke, Quadrate und Kreise.
Die Formen unterscheiden sich in Größe und Position auf dem Bild.
Andere Transformationen, wie beispielsweise Rotationen, werden nicht angewendet.
\newline

Die Quadrate und gleichseitigen Dreiecke sind zwischen 5 und 16 Pixel groß.
Die Größe bezieht sich dabei auf die Seitenlänge der Seiten.
Die Kreise sind zwischen 5 und 10 Pixel groß.
Hierbei gibt der Wert den Radius des Kreises an.
\newline

Die Formen sind so im Bild positioniert, dass sie Seitenränder touchieren können.
Allerdings ist eine Form immer vollständig im Bild abgebildet und wird vom Bildrand nicht geschnitten.

\newcommand{\trainDataImage}[1]{\subfloat{\fbox{\includegraphics{#1}}}}
\begin{figure}[H]
	\centering
	\trainDataImage{kapitel/3\_methodik/data/circle\_00.png}
	\trainDataImage{kapitel/3\_methodik/data/circle\_01.png}
	\trainDataImage{kapitel/3\_methodik/data/circle\_02.png}\par 
	
	\trainDataImage{kapitel/3\_methodik/data/rectangle\_00.png}
	\trainDataImage{kapitel/3\_methodik/data/rectangle\_01.png}
	\trainDataImage{kapitel/3\_methodik/data/rectangle\_02.png}\par 
	
	\trainDataImage{kapitel/3\_methodik/data/triangle\_00.png}
	\trainDataImage{kapitel/3\_methodik/data/triangle\_01.png}
	\trainDataImage{kapitel/3\_methodik/data/triangle\_02.png}\par 
	
	\caption{Auswahl an zufällig generierten Trainingsbilder}
\end{figure}

\section{Architekturen}
\subsection{API}
Um alle möglichen Architekturen in das Training einbinden zu können müssen sie die gleichen Ein- und Ausgaben besitzen.
Die Anforderungen an die Ein- und Ausgabe wie folgt definiert.
\newline

Der Generator erhält als Eingabe einen 100 Stellen langes Array mit Zufallszahlen (Latent-Dimension) und eine Zahl zwischen 0 und 2, die das Label repräsentiert.
Die Ausgabe des Generators ist ein Array mit den gleichen Dimensionen, wie ein Bild (28x28x1).
\newline

Der Discriminator erhält als Eingabe ein Bild als Array und zusätzlich ein Label als Zahl.
Die Ausgabe ist eine Zahl zwischen 1 und -1, die die vorhergesagte Echtheit angibt.

\section{Verfahren zur Hyperparametersuche}
\todo[shadow,inline]{Aufbau ändern in: Kriterien $\rightarrow$ Tabelle $\rightarrow$ Auswahl/Bewertung (siehe Kommentare)}
Zur Auswahl von Hyperparametern gibt es verschiedene Verfahren.
Dazu gehören unter anderem die Manuelle Suche, Grid Search, Random Search und Genetic Algorithms (siehe \cref{chapter:verfahren-bestimmung-hyperparameter}).
In diesem Abschnitt werden die verschiedenen Verfahren verglichen und zusätzliche Anpassungen erläutert.

\subsection{Kriterien}
\todo[inline, shadow]{irgendwo hardware festhalten}
In diesem Abschnitt werden Kriterien für die Auswahl eines geeigneten Hyperparameter Suchverfahrens ausgewählt.
Die Kriterien dienen als Basis für die Bewertung der Suchverfahren im nächsten Schritt.

\paragraphNewLine{Automatisierbarkeit}
Ziel ist es, pro Nacht eine Architektur zu trainieren.
Deswegen ist es wichtig, dass das Verfahren vollkommen automatisiert ablaufen kann.

\paragraphNewLine{Ressourcenbedarf}
Das Verfahren darf in keinem Fall während oder nach einer Trainingsiteration so viel zusätzlichen Speicher belegen, dass das Training fehlschlägt.
Denn Trainingsabläufe sollen primär über Nacht durchgeführt werden, wodurch Fehlschläge erst sehr spät entdeckt werden.

Dabei ist zu beachten, dass die Hardware den Trainingsprozess stark beeinflusst.
So ist der verfügbare Arbeits- und Grafikkartenspeicher nicht ausreichend, um Trainingsdurchläufe stark zu parallelisieren.

\paragraphNewLine{Laufzeit}
Die Laufzeit der Hyperparametersuche ist von entscheidender Bedeutung.
Denn die verwendete Hardware ist nicht ausreichend, um jegliche Kombinationen von Hyperparametern in einer vernünftigen Zeit zu testen.
Als zeitliche Begrenzung dient dabei die Ausführung in einer Nacht von 8h.

Jedoch ist zu beachten, dass auch der Trainingsprozess als solcher viel Zeit in Anspruch nimmt.
Der reine Trainingsaufwand erlaubt mit der verwendeten Hardware in den 8h maximal 75 sequentielle Trainingsdurchläufe (je nach Architektur kann der genaue Wert abweichen).
Folglich sollte das Verfahren innerhalb von 75 Durchläufen zu einem Ergebnis kommen.

\paragraphNewLine{Effektivität}
Das Verfahren soll in jedem Fall eine gute Kombination im zeitlichen Rahmen finden.
Das ist insofern wichtig, als dass ansonsten kein Vergleich zu anderen Architekturen erfolgen kann.
Aufgrund der geringen Ressourcen (Zeit und Rechenleistung) besteht zwar kein Anspruch auf Optimalität, jedoch sollte es zumindest ein gutes Ergebnis liefern.

\paragraphNewLine{Implementierungsaufwand}
Der zusätzlich verursachte Implementierungsaufwand ist nicht so wichtig wie die anderen Kriterien.
Er kann aber als Ausschlusskriterium bei ähnlich guten Verfahren dienen.
Der Implementierungsaufwand wird durch Dokumentation, Tutorials, Libraries, oder eine Einbindung in Tensorflow/-board verringert.

Bei der Verwendung von Libraries ist essentiell, dass sie eigene Trainingsfunktionen und den spezielleren Trainingsablauf von GANs unterstützen.
Zusätzlich müssen sie in jedem Fall mit Tensorflow oder Keras kompatibel sein.

\subsection{Vergleich}
In diesem Abschnitt werden die vorgestellten Verfahren miteinander auf Basis der Kriterien verglichen.

\paragraphNewLine{Manuelle Suche}
Die manuelle Suche ist aufgrund der ständigen verpflichtenden Neuevaluierungen der Traingsergebnisse als einziges Verfahren \todo{reicht es nur automatisierbarkeit nur in der manuellen suche anzusprechen?} nicht automatisierbar.
Zudem erhöhen die Evaluierungen den Zeitaufwand zusätzlich, da jedes mal eine fundierte\todo{mehr richtung durchdachte?} Entscheidung getroffen werden muss.
Insgesamt erhöht sich so die Laufzeit im Vergleich zu automatisierten Verfahren sehr stark.
\newline

Alle anderen Kriterien werden hingegen optimal erfüllt.
Denn für die Einführung einer manuellen Suche müssen keinerlei Anpassungen am Trainingsprozess oder Code vorgenommen werden.

\paragraphNewLine{Grid Search}
Die Laufzeit der Grid Search hängt stark von der Anzahl der Hyperparameter ab.
Das liegt an der Durchführung eines Trainings für jede mögliche Kombination aus den Hyperparametern.
Im Rahmen der Studienarbeit werden jedoch nur wenige bestimmte Hyperparameter getestet, wodurch eine ausreichend schnelle Laufzeit gegeben ist.
Zwar könnte das Verfahren insgesamt durch Parallelisierung noch beschleunigt werden, das würde allerdings die Hardwarekapazitäten übersteigen.
Bei einem sequentiellen Ablauf stellt die Ressourcennutzung keine Probleme dar.
\newline

Die Effektivität des Verfahrens ist auch gegeben, solange eine geeignete Kombination aus den Hyperparameterwerten gebildet werden kann.
Sollte keine gute Kombination im Suchraum existieren, scheitert das Verfahren.
Dementsprechend wichtig ist die Wahl der Werte.
Die Effektivität kann allerdings mit Verfahrensanpassungen sogar erhöht werden.
Zwar findet die Grid Search in keiner intelligenten Weise die optimalen Hyperparameter, aber durch kurze Trainings- und Validierungsintervalle können schlecht trainierende Kombinationen schnell aussortiert werden.
Das erlaubt ein viel größeren Suchraum und somit auch eine effektivere Suche.
\newline

Die Implementierung in das Projekt ist sehr einfach, da eine Grid Search native von Tensorflow unterstützt wird.
Durch eine Iteration über das Kreuzprodukts aller Hyperparameterwerte lässt sich dann die Suche umsetzen.
Durch den simplen Neustart des Trainings werden auch automatisch eigene Trainingsfunktionen oder -abläufe unterstützt.

\paragraphNewLine{Random Search}
Die Random Search ist der Grid Search sowohl im Verhalten als auch in der Bewertung sehr ähnlich.
Auch bei der Random Search sind Ressourcenbedarf und Laufzeit vertretbar, solange das Verfahren nicht parallelisiert wird und nicht zu viele zufällige Werte für die Hyperparameter ausgewählt werden.
\newline

Statt der Wahl der Werte beeinflusst bei diesem Verfahren die Wahl der Intervalle die Effektivität.
Diese müssen so klein gewählt werden, dass bei der Anzahl an generierten Werten eine Chance besteht, optimale Werte zu finden.
Das bedeutet einem Intervall von 100 möglichen Werten sollte nicht nur 1 Zufallswert gegenüberstehen. \todo{in Stand der Technik?}
\newline

Der Implementierungsaufwand ist etwas größer als bei der Grid Search.
Zwar gibt es das Package KerasTuner \cite{omalley2019kerastuner}.
Der KerasTuner hat allerdings keine Dokumentation zu eigenen Trainingsabläufen/-funktionen, die beim GAN zum Einsatz kommen.
Allerdings können die Werte auch ohne den KerasTuner erzeugt und dann wie bei der Grid Search verwendet werden.
Dann fehlt jedoch eine Funktion, die eine \textit{Automatic Relevance Determination} über die Werte ausführt.

\paragraphNewLine{Genetic Algorithm}
Die Laufzeit des Genetic Algorithms ist bei wenigen Hyperparametern vergleichsweise hoch.
Dafür nimmt die Laufzeit bei mehr Hyperparametern in nur geringem Maß zu.
Jedoch wird für die Studienarbeit eine stark begrenzende Vorauswahl an Hyperparametern getroffen. \todo{Wo begründet?}
Dadurch ist die Laufzeit des Verfahrens vergleichsweise hoch.

Durch eine mangelnde Parallelisierung müssen auch alle Modelle in einem Evolutionsschritt sequentiell trainiert werden.
Da es sich dabei beispielsweise um 50 (Standardwert für Tensorflow Optimierung) parallele Modelle handelt, ist steigert das die Laufzeit sehr stark.
\newline

Die Effektivität des Verfahrens ist gegeben, wenn es komplett durchlaufen kann.
Das ist aufgrund der hohen Laufzeit allerdings fraglich.
Sobald das Verfahren früher abgebrochen wird, muss keine Effektivität mehr vorhanden sein.
Insbesondere unter der Annahme, dass sich die Hyperparameter stetig anpassen und optimieren \todo{und das netz nicht übertrainiert wurde}, sollte sogar ein deutlich schlechteres Ergebnis geliefert werden, als am Ende herauskommt.
\newline

Für die Implementierung stellt Tensorflow eine Optimierungsfunktion bereit.
Jedoch unterstützt die Funktion keine eigenen Trainingsabläufe, oder es ist nicht dokumentiert.
Aufgrund der fehlenden Informationslage ist der tatsächliche Aufwand nur schwer einschätzbar. \todo{nochmal recherchieren?}
Er sollte jedoch deutlich höher liegen, als bei den anderen beiden Verfahren.

\subsection{Bewertung}
Die manuelle Suche ist viel zu aufwändig und für moderne Suchen nicht mehr zeitgemäß, da eine Automatisierung fehlt.
Für eine derart intensive Suche fehlen die Ressourcen und der dabei entstehende Aufwand wiegt auch nicht den Nutzen auf.
\newline

Der Genetic Algorithm ist zwar an sich ein sehr guter Algorithmus.
Allerdings ist das Projekt zu klein und die Hardware zu beschränkt, um ihn richtig ausnutzen zu können.
Bei größeren Projekten und vor allem bei weniger ressourcenaufwändigen Trainingsdurchläufen als Bildgenerierungen mithilfe von GANs ist er allerdings definitiv vorzuziehen.
Das liegt vor allem an der allgemeinen Suchart, bei der jegliche Kombination und diverse Hyperparameter in Betracht gezogen werden können.
\newline

Sowohl die Grid Search als auch die Random Search sind geeignete Verfahren im Rahmen dieser Arbeit.
Sie sind sich so ähnlich, dass auch ihre Vorteile und Nachteile sich nicht stark unterscheiden.
Die Verfahren sind vor allem aufgrund ihrer geringen Laufzeit bei wenigen Hyperparametern und der trotzdem guten Effektivität gewählt.

Ausschlaggebend unterscheiden tun sich die Verfahren nur in der Implementierung.
Hierbei ist die Grid Search der Random Search etwas überlegen und wird deshalb implementiert.

\section{Training}
Im Trainingsprozess werden verschiedene Architekturen mit unterschiedlichen Hyperparametern getestet.
Die ausgewählten Architekturen orientieren sich an bereits existierender Forschung und Tutorials (siehe \cref{chapter:related-work}).
Insbesondere aufgrund der Label-Abhängigkeit (im Rahmen der Studienarbeit entsteht ein Conditional-GAN) werden jedoch Anpassungen erfolgen.


\subsection{Auswahl zu optimierender Hyperparameter}
\todo[inline, shadow]{überarbeiten}
Es ist nicht möglich, alle möglichen Hyperparameter zu optimieren.
Eine solche Optimierung benötigt zu viele Ressourcen, die im Rahmen der Studienarbeit nicht vorhanden sind.
Deshalb wird die Optimierung auf einige vielversprechende Hyperparameter beschränkt.
\newline

Die auszuwählenden Hyperparameter sollen dabei zum einen eine vergleichsweise hohe Auswirkung auf den Trainingserfolg haben.
Zum anderen sollen sie sehr allgemein sein, um sie gleichermaßen auf die verschiedenen Architekturen anwenden zu können.
Das garantiert eine bessere Vergleichbarkeit zwischen den Architekturen.
\newline

Die Hyperparameter \textit{Batch-Size}, \textit{Learning-Rate Generator}, \textit{Learning-Rate Discriminator}, \textit{Dropout} und \textit{Smoothness} entsprechen diesen Voraussetzungen.
\newline

Die Learning-Rate hat allgemein einen sehr hohen Einfluss und sollte immer angepasst werden \cite{learning-rate-most-important}.
Ähnlich wie die Learning-Rate ist auch die Batch-Size ein sehr beliebter Hyperparameter für Anpassungen und wird in unterschiedlichen Tutorials angegeben \cite{tutorial:tune-batch-size-analyticsvidhya, tutorial:tune-batch-size-machinelearningmastery} \todo{Begründung aus der quelle übernehmen? \cite{tutorial:tune-batch-size-machinelearningmastery}}.
Sowohl Dropout als auch Smoothness sind vor allem für einen persönlichen Erkenntnisgewinn mit aufgenommen worden.
Die Smoothness entstammt einer Idee von Ian Goodfellow \cite{ian-goodfellow-onesided-label-smoothing}.

\subsection{Trainingsablauf}
Vor der Implementierung und Analyse der verschiedenen Architekturen werden zunächst die Trainingsdaten generiert.
Danach erfolgt die Implementierung einer neuen Architektur und Festlegung der Hyperparameter.
Schließlich wird das Training durchgeführt.
Beim Training werden automatisch alle Hyperparameter-Kombinationen ausprobiert, Log-Dateien erstellt und auch die FID-Werte für die einzelnen Durchgänge berechnet.
Alle Metriken in den Logs werden dabei auf Testdaten angewendet, um ein Overfitting erkennen zu können.
Die Logs können im Anschluss mithilfe von Tensorboard visualisiert werden.

\section{Bewertung}
\subsection{Erfolgskriterien}
Damit die Konfigurationen treffend vorausgewählt werden könne, müssen die Kriterien für ein erfolgreich trainiertes GAN klar definiert werden.
Als Erfolgskriterien zählen in diesem Fall die Varietät der generierten Bilder und die Korrektheit der generierten Figuren.
Beide Kriterien werden im Folgenden noch einmal genauer erläutert.

\paragraphNewLine{Varietät}
Die Varietät bezieht zum einen auf die Ähnlichkeit zwischen Trainingsbildern und den generierten Bildern der GANs.
Die Ähnlichkeit zwischen diesen beiden Bildersets beschreibt, wie gut das GAN 'etwas neues schaffen' kann, oder ob es nur Trainingsbilder dupliziert.
Sollte die Ähnlichkeit sehr gering sein, werden viele 'neue' Bilder generiert, was sehr positiv zu bewerten ist.
Zudem bezieht sich die Varietät auf die generierten Bilder untereinander.
Sie sollten auch möglichst verschieden sein, was oftmals nicht der Fall ist.
Das Phänomen ist als 'mode-collapse' bekannt (siehe Stand der Technik). %TODO steht das auch in Stand der Technik?

Beide Probleme werden mittels einer manuellen Analyse ausgewertet.
% https://scikit-image.org/docs/0.12.x/api/skimage.measure.html#skimage.measure.compare_ssim

\paragraphNewLine{Korrektheit}
Neben der Varietät besitzt auch die Korrektheit eine hohe Bedeutung für die Bewertung der GANs.
Dabei muss sichergestellt werden, dass die richtige Figur generiert wurde und erkennbar ist.
Die Figuren müssen außerdem den gleichen Kriterien wie die Trainingsdaten genügen, das heißt, die Figuren sollten zum Beispiel vollständig innerhalb des Bildes abgebildet sein.
\todo{markus: wertung}
Es ist allerdings eher unwahrscheinlich, dass das GAN Bilder generiert, die keinem Pendant aus den Trainingsbildern entsprechen.
Ein weiterer wichtiger Aspekt der Bilder ist das Verhalten im Hintergrund der Figur.
So sollten im Hintergrund möglichst keine anderen Figuren oder Pixelfragmente erzeugt werden.

All diese Kriterien werden durch eine manuelle Überprüfung evaluiert werden.

\begin{itemize}
	\item manuelle Auswertung
	\item Frechet Inception Distance \cite{frechet-inception-distance}
\end{itemize}

\subsection{Logs}
Während des Trainings wird eine Reihe von Logs zur Analyse erzeugt.
Die Logs werden bei der Erzeugung automatisch in Architektur und Hyperparameterkombination unterteilt.
Durch Unterteilung ist die Grundlage einer fundierten Auswahl einer Architektur mit den besten Ergebnissen sichergestellt.
\newline

Während dem Training werden mehrere Werte geloggt.
Ein Wert ist dabei der Loss, sowohl für den Generator, als auch für den Discriminator.
Der Loss wird dabei für jede Epoche geloggt und ist vor allem zur Analyse des Trainingsprozesses interessant.
So können nicht funktionierende Architekturen schon früher abgebrochen werden und verbessert werden.
\newline

Für die Auswertung sind vor allem die generierten Bilder und der FID-Index relevant.
Die Bilder werden nach jeder Epoche mit einer immer identischen Latent-Dim erzeugt.
Dadurch ist eine bessere Vergleichbarkeit gewährleistet.
Tensorboard erlaubt dann eine Ansicht mit Zeitachse, um den Lernprozess über die verschiedenen Epochen beobachten zu können.
\newline

Der FID-Index ist sehr aufwändig zu berechnen.
Deshalb wird er erst am Ende mit den finalen Bildern erzeugt.
Bei einem Overfitting könnten so theoretisch schlechtere Ergebnisse für ein Modell entstehen.
Durch eine zusätzliche manuelle Analyse über die einzelnen Epochen und der vergleichsweise geringen Epochenanzahl ist das Risiko einer potentiellen Verfälschung der Ergebnisse jedoch sehr gering.
\newline

Die gesammelten Daten sind dabei alle nach Architektur und Hyperparameterkombination unterteilt.
Die Unterteilung erlaubt die Auswahl einer möglichst optimalen Kombination aus beidem.


\begin{comment}

Die Korrektheit bezieht sich dabei auf die generierte Figur im Bild oder auch umliegende Bildpunkte.
Für die Korrektur der richtigen Formen gibt es mehrere Möglichkeiten:
\begin{description}
	\item[Neuronales Kontrollnetz]
	Dafür wird ein weiteres Netzwerk zur Bewertung der Resultate trainiert.
	Diese Variante ist sehr ungenau, da sie wieder von dem Trainingserfolg eines Neuronalen Netzes abhängt.
	Allerdings erlaubt das Neuronale Netzwerk die Analyse von großen Datensätzen, die aber in diesem Fall nicht in der Form nötig sein wird.
	
	\item[händisch]
	Das händische Kontrollieren ist sehr aufwändig.
	Für die Kontrolle sollte das aber möglich sein.
	
	\item[Formvergleich]
	Es ist möglich, die Lösung zu brute-forcen.
	Das bedeutet, es wird jede mögliche Form über das Bild gelegt und diejenige mit der höchsten Überschneidung ausgewählt und als Indikator genommen.
\end{description}



\section{GAN: Bewertungs- und Vergleichskriterien}

% How to write methods section: http://rc.rcjournal.com/content/respcare/49/10/1229.full.pdf
\subsection{Warum synthetische Daten?}
Mehr Kontrolle über die Bilder:
\footnote{Die meisten Anpassungen erlauben auch Rechenleistung zu reduzieren, falls Computer überlastet sein sollten.}
\begin{enumerate}
	\item Anpassung von Größe
	\item Anzahl der unterschiedlichen labels (wieviel unterschiedliche Formen sind enthalten)
	\item sich unterscheidende Features in den Bildern (wo genau sind die Bilder zu finden, unterschiedliche Größe, Rotation?)
	\item wie generiert? $\rightarrow$ Deterministisch $\rightarrow$ Seed
	\item weniger Arbeitsaufwand als händische Generierung
\end{enumerate}

\subsection{Welche Eigenschaften haben die Bilder genau?}
\begin{enumerate}
	\item größe: 28x28
	\item farbe: 1-dimensional $\rightarrow$ scharz-weiß 
\end{enumerate}

\section{Training}
Ziel: GAN das möglichst diverse und korrekte Bilder erzeugt $\rightarrow$ Wie wird das erreicht?
\begin{enumerate}
	\item Anpassung von Hyperparametern (Learning-rate, batch-size, momentum)
	\footnote{https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a}
	\footnote{Grid Search (erlaubt methodisches Suchen nach optimalen Hyperparametern): https://www.tensorflow.org/tensorboard/hyperparameter\_tuning\_with\_hparams}
	\item Anpassung von Neuralen Netzarchitektur
	\footnote{https://lab.wallarm.com/the-first-step-by-step-guide-for-implementing-neural-architecture-search-with-reinforcement-learning-using-tensorflow-99ade71b3d28/}
\end{enumerate}

\section{Bewertung bzw. Vergleich der GANs}
Ziel: messbare Vergleichskriterien

\subsection{Korrektheit der Bilder}
\begin{enumerate}
	\item richtige Form für das angegebene Label? 
	\item muss per Hand bestimmt werden? oder festen algorithmus, der form erkennt? neuronales netz wäre zu ungenau?
\end{enumerate}

\subsection{Diversität der generierten Bilder}
\begin{enumerate}
	\item Vergleich zu Trainingsdaten (wird etwas neues geschaffen?)
	\item Vergleich zu anderen generierten Bildern $\rightarrow$ (Parital) Mode-Collapse?
	\footnote{https://developers.google.com/machine-learning/gan/problems}
	\footnote{eventuell 2 discriminator?: https://dl.acm.org/doi/10.1145/3283254.3283282}
	\footnote{Google: https://research.google/pubs/pub45829/}
	
	\item Vergleich von Bildern durch 'Pixel by Pixel' Ähnlichkeit bestimmen $\rightarrow$ die X bilder mit einer Ähnlichkeit über Y \% können dann angeguckt werden
\end{enumerate}
\end{comment}